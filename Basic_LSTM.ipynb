{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Image Captioning   : Done\n",
    "Sentiment Analysis : Done\n",
    "\n",
    "Many-to-Many       : Find out how to do the 2 different kind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic LSTMs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "One-to-One:\n",
    "    We dont use this, no difference than a normal feedforward network.\n",
    "\n",
    "1) One-to-Many:\n",
    "    Image Captioning:\n",
    "        One  : Image as is, OR\n",
    "               Image coded by CNN, then fed to RNN as a vector.\n",
    "        Many : Let RNN output captions by rolling out indefinitely. \n",
    "\n",
    "2) Many-to-One:\n",
    "    Sentiment Analysis:\n",
    "        Many : The text words embedded into many vectors, ordered.\n",
    "        One  : Output is the sentiment.\n",
    "            Method 1: Sum/Concat of all unrolled-RNN output to classifiation layer.\n",
    "            Method 2:             Use only final RNN output to classifiation layer.\n",
    "\n",
    "3) Many-to-Many:\n",
    "    Version 1: Finish reading the sequence first. Then use final RNN-state to output/start another sequence.\n",
    "        Sequence Prediction:\n",
    "            Many(Input)  : [0,1,2,3]\n",
    "            Many(Output) :          [4,5,6,7]\n",
    "        Machine Translation:\n",
    "            Many(Input)  : ['I','want']\n",
    "            Many(Output) :             ['我','要']\n",
    "\n",
    "    Version 2: For each input to the RNN, there will be output.\n",
    "        Memorizing hello:\n",
    "            Many(Input)  : ['h','e','l','l']\n",
    "            Many(Output) :     ['e','l','l','o']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "## To get helper functions from another folder\n",
    "# sys.path.insert(0, '../') # if _helper_basics_ is in previous folder\n",
    "now_i_am_at = 'home' # home dso test\n",
    "if now_i_am_at=='home':\n",
    "    sys.path.insert(0, 'E:/Leonard HDD/Dropbox/DSO/Tasks/')\n",
    "elif now_i_am_at=='dso':\n",
    "    sys.path.insert(0, 'D:/Dropbox/DSO/Tasks')\n",
    "\n",
    "from _helper_basics_ import *\n",
    "\n",
    "## Deep learning\n",
    "# import tensorflow as tf\n",
    "# import keras\n",
    "# from keras.models import Model, Sequential\n",
    "# from keras.layers import *\n",
    "# from keras import optimizers\n",
    "# import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras 2.0.5\n",
      "tensorflow 1.2.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['savefig.dpi'] = 100\n",
    "\n",
    "## Why need autoreload - https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print('keras', keras.__version__)\n",
    "print('tensorflow', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-to-Many LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Captioning\n",
    "2 Versions, whether Image/Image_code as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Image_Code (InputLayer)      (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "Repeat (RepeatVector)        (None, 3, 10)             0         \n",
      "_________________________________________________________________\n",
      "RNN_1 (LSTM)                 (None, 3, 2)              104       \n",
      "=================================================================\n",
      "Total params: 104\n",
      "Trainable params: 104\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 3s - loss: 0.1880\n",
      "X:\n",
      " [ 0.06110775  0.90476344  0.22743712  0.53567283  0.59140566  0.64479753\n",
      "  0.63045467  0.7113344   0.90057832  0.80999393]\n",
      "result:\n",
      " [[ 0.2347582   0.31728387  0.34880367]\n",
      " [ 0.04969562  0.08802228  0.11605316]]\n",
      "y:\n",
      " [[ 0.06333702  0.78159723  0.58280625]\n",
      " [ 0.85212739  0.4986673   0.20912442]]\n"
     ]
    }
   ],
   "source": [
    "def set_X(Input_type, m,n,channel, num_feat_inp):\n",
    "    if Input_type == 'image':\n",
    "        ## Input : Image\n",
    "        # (None, m, n, channel)\n",
    "        #    m,n        : width,height of image\n",
    "        #    channel    : B/W or Color\n",
    "        m,n = 5,5\n",
    "        channel = 1\n",
    "        X = np.random.rand(1, m,n, channel)\n",
    "        X = X.reshape(1, m,n, channel)\n",
    "    elif Input_type == 'embedding':\n",
    "        ## Input : Embedding from a ConvNet/etc\n",
    "        # input for LSTMs must be three dimensional.\n",
    "        # (None, num_feat_inp)\n",
    "        #    num_feat_inp : lvl of embedding\n",
    "        X = np.random.rand(1, num_feat_inp)\n",
    "        X = X.reshape(1, num_feat_inp)\n",
    "    return X\n",
    "def set_Archi_1(Input_type, m,n,channel, num_feat_inp):\n",
    "    if Input_type == 'image':\n",
    "        ## Input is a image\n",
    "        Inp = Input( shape=(m,n, channel),name = \"Image\" )\n",
    "        ######################## Image_Code (ConvNet) #######################\n",
    "        # x = ResNet50(weights='imagenet')(Inp)\n",
    "        x = Conv2D(64, (3,3), activation='relu', padding='same', name='Conv2D_1')(Inp)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(num_feat_inp, name='Image_Code')(x)\n",
    "        #####################################################################\n",
    "    elif Input_type == 'embedding':\n",
    "        Inp = Input( shape=(num_feat_inp,), name = \"Image_Code\" )\n",
    "        x = Inp\n",
    "    return Inp, x\n",
    "\n",
    "Input_type = 'embedding' #'embedding' 'image'\n",
    "X = set_X(Input_type, m,n,channel, num_feat_inp)\n",
    "## Output : Caption\n",
    "# (None, num_words, num_feat_out)\n",
    "#    num_words    : num of words in a sentence.\n",
    "#    num_feat_out : lvl of embedding\n",
    "num_feat_inp = 10 # For Image Code/Embedding\n",
    "num_words    = 3\n",
    "num_feat_out = 2\n",
    "y = np.random.rand(1, num_words, num_feat_out)\n",
    "y = y.reshape(1, num_words, num_feat_out)\n",
    "\n",
    "########################### Architecture ##################################\n",
    "Inp, x = set_Archi_1(Input_type, m,n,channel, num_feat_inp)\n",
    "########################### Recurrent Section ###########################\n",
    "## RepeatVector to tally the number of words output.\n",
    "x = RepeatVector(num_words, name='Repeat')(x)\n",
    "x = LSTM(num_feat_out, name = \"RNN_1\", return_sequences=True)(x)\n",
    "########################### Printing information ########################\n",
    "model = Model(Inp, x)\n",
    "model.summary()\n",
    "# print_model_weights(model)\n",
    "################################# Training ##############################\n",
    "n_batch = 1\n",
    "n_epoch = 1\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X, y, epochs=n_epoch, batch_size=n_batch, verbose=1)\n",
    "############################### Evaluation #############################\n",
    "result = model.predict(X)\n",
    "print('X:\\n',X[0].T)\n",
    "print('result:\\n',result[0].T)\n",
    "print('y:\\n',y[0].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Many-to-One LSTM for Sequence Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           (None, 5, 1)              0         \n",
      "_________________________________________________________________\n",
      "RNN_1 (LSTM)                 (None, 2)                 32        \n",
      "=================================================================\n",
      "Total params: 32\n",
      "Trainable params: 32\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 2s - loss: 0.7656\n",
      "X:\n",
      " [[ 0.   0.2  0.4  0.6  0.8]]\n",
      "result:\n",
      " [ 0.25404558 -0.20936279]\n",
      "y:\n",
      " [0 1]\n"
     ]
    }
   ],
   "source": [
    "length = 5\n",
    "seq = np.array([i/float(length) for i in range(length)])\n",
    "class_type = np.array([0,1])\n",
    "## Input \n",
    "# The input for LSTMs must be three dimensional.\n",
    "# (None, length, feature)\n",
    "#    None       : len(data)\n",
    "#    length     : duration of speech,text\n",
    "#    feature    : lvl of embedding\n",
    "X = seq.reshape(1, length, 1)\n",
    "## Output \n",
    "# (None, feature)\n",
    "#    None       : len(data)\n",
    "#    feature    : lvl of embedding\n",
    "y = class_type.reshape(1, 2)\n",
    "\n",
    "n_neurons = length\n",
    "########################### Architecture ##################################\n",
    "Inp = Input( shape=(5,1),name = \"Input\" )\n",
    "x = LSTM(2, name = \"RNN_1\")(Inp) \n",
    "# x = Dense(5,name = \"Output\" )(x)\n",
    "model = Model(Inp, x)\n",
    "########################### Printing information ########################\n",
    "model.summary()\n",
    "# print_model_weights(model)\n",
    "############################### Evaluation #############################\n",
    "n_batch = 1\n",
    "n_epoch = 1\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X, y, epochs=n_epoch, batch_size=n_batch, verbose=1)\n",
    "############################### Evaluation #############################\n",
    "result = model.predict(X, batch_size=1, verbose=0)\n",
    "print('X:\\n',X[0].T)\n",
    "print('result:\\n',result[0].T)\n",
    "print('y:\\n',y[0].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Many-to-Many RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Prediction\n",
    "            Many(Input)  : [0,1,2,3]\n",
    "            Many(Output) :          [4,5,6,7]\n",
    "## Machine Translation\n",
    "            Many(Input)  : ['I','want']\n",
    "            Many(Output) :             ['我','要']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           (None, 4, 1)              0         \n",
      "_________________________________________________________________\n",
      "Encoder (LSTM)               (None, 10)                480       \n",
      "_________________________________________________________________\n",
      "Repeat (RepeatVector)        (None, 4, 10)             0         \n",
      "_________________________________________________________________\n",
      "Decoder_1 (LSTM)             (None, 4, 10)             840       \n",
      "_________________________________________________________________\n",
      "Decoder_2 (LSTM)             (None, 4, 1)              48        \n",
      "=================================================================\n",
      "Total params: 1,368\n",
      "Trainable params: 1,368\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 5s - loss: 31.5729\n",
      "X:\n",
      " [0 1 2 3]\n",
      "predict:\n",
      " [-0.0019764  -0.00380387 -0.00473887 -0.00476281]\n",
      "y:\n",
      " [4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "embedding     = 1\n",
    "num_words_in  = 4\n",
    "LSTM_neurons  = 10\n",
    "num_words_out = 4\n",
    "X = np.array([0,1,2,3]).reshape(1,num_words_in, embedding)\n",
    "y = np.array([4,5,6,7]).reshape(1,num_words_out,embedding)\n",
    "########################### Architecture ##################################\n",
    "Inp = Input( shape=(num_words_in, embedding),name = \"Input\" )\n",
    "x = LSTM(LSTM_neurons, name = \"Encoder\")(Inp)\n",
    "\n",
    "x = RepeatVector(num_words_out, name='Repeat')(x)\n",
    "\n",
    "x = LSTM(LSTM_neurons, name = \"Decoder_1\", return_sequences=True)(x)\n",
    "x = LSTM(embedding, name = \"Decoder_2\", return_sequences=True)(x)\n",
    "########################### Printing information ########################\n",
    "model = Model(inputs=Inp, outputs=x)\n",
    "model.summary()\n",
    "# print_model_weights(model)\n",
    "\n",
    "n_batch = 1\n",
    "n_epoch = 1\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X, y, epochs=n_epoch, batch_size=n_batch, verbose=1)\n",
    "print('X:\\n',X.reshape(num_words_in))\n",
    "print('predict:\\n', model.predict(X.reshape(1,num_words_in,embedding)).reshape(num_words_out).T)\n",
    "print('y:\\n',y.reshape(num_words_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completing Last Char\n",
    "    Many(Input)  : ['h','e','l','l']\n",
    "    Many(Output) :     ['e','l','l','o']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           (None, 4, 1)              0         \n",
      "_________________________________________________________________\n",
      "RNN_1 (LSTM)                 (None, 4, 99)             39996     \n",
      "_________________________________________________________________\n",
      "Output (LSTM)                (None, 4, 1)              404       \n",
      "=================================================================\n",
      "Total params: 40,400\n",
      "Trainable params: 40,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 3s - loss: 7.4781\n",
      "result:\n",
      " [[ 0.00098981  0.00435242  0.01138499  0.02222412]]\n",
      "y:\n",
      " [[1 2 3 4]]\n"
     ]
    }
   ],
   "source": [
    "num_chars = 4\n",
    "num_features = 1\n",
    "num_neurons = 99\n",
    "hello = np.array([0,1,2,3,4])\n",
    "X = hello[:4].reshape(1,num_chars,num_features)\n",
    "y = hello[1:].reshape(1,num_chars,num_features)\n",
    "\n",
    "## This Architecture is assuming 1 input\n",
    "n_neurons = 100\n",
    "num_channel = 1 # this is just 1, when using conv layers this dimension is to fit filter=64, etc\n",
    "########################### Architecture ##################################\n",
    "Inp = Input( shape=( num_chars,num_features),name = \"Input\" )\n",
    "## Intermediate layers\n",
    "x = LSTM(num_neurons, name = \"RNN_1\", return_sequences=True)(Inp) \n",
    "# x = LSTM(num_neurons, name = \"RNN_2\", return_sequences=True)(x) \n",
    "# x = LSTM(n_neurons, name = \"RNN_2\")(x) \n",
    "## Output layer\n",
    "x = LSTM(num_features, activation='linear', name = \"Output\", return_sequences=True)(x) \n",
    "model = Model(Inp, x)\n",
    "########################### Printing information ########################\n",
    "model.summary()\n",
    "# print_model_weights(model)\n",
    "################################# Training ##############################\n",
    "n_batch = 1\n",
    "n_epoch = 1\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X, y, epochs=n_epoch, batch_size=n_batch, verbose=1)\n",
    "################################ Evaluation #############################\n",
    "result = model.predict(X)\n",
    "print('result:\\n',result[0].T)\n",
    "print('y:\\n',y[0].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Captioning\n",
    "    Many(Input)  : ['frame_1','frame_2','frame_3','frame_4']\n",
    "    Encode_Layer : [ 'code_1', 'code_2', 'code_3', 'code_4']\n",
    "    Many(Output) :                                  ['I','eat','rice','.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_frames, m,n,channel = 5, 10,10, 3\n",
    "num_words, num_features = 5, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 5, 10, 10, 3)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_22 (TimeDis (None, 5, 64)             411456    \n",
      "_________________________________________________________________\n",
      "lstm_27 (LSTM)               (None, 2)                 536       \n",
      "=================================================================\n",
      "Total params: 411,992\n",
      "Trainable params: 411,992\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Input_type = 'image' # 'embedding'\n",
    "\n",
    "## EDIT : it doesnt seem to tally.\n",
    "\n",
    "if Input_type == 'image':\n",
    "    Inp = Input( shape=(m,n, channel),name = \"Input\" )\n",
    "    x = Conv2D(64, (3,3), activation='relu', padding='same')(Inp)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, name='Conv_Mod_Embedding')(x)\n",
    "    Conv_Model = Model(Inp ,x)\n",
    "    ########################### Printing information ########################\n",
    "#     Conv_Model.summary()\n",
    "    # print_model_weights(Conv_Model)\n",
    "\n",
    "    num_timestep = 5\n",
    "    audio_input = Input(shape=(num_timestep, m,n, channel))\n",
    "    encoded_frame_sequence = TimeDistributed(Conv_Model)(audio_input)  \n",
    "    x = LSTM(2)(encoded_frame_sequence)  # the output will be a vector\n",
    "    Video_Captioning_Model = Model(inputs=audio_input, outputs=x)\n",
    "\n",
    "\n",
    "# elif Input_type == 'embedding':\n",
    "#     Inp = Input( shape=(1,num_features),name = \"Input\" )\n",
    "#     x = LSTM(num_features, name = \"RNN_1\")(Inp) \n",
    "#     Video_Captioning_Model = Model(Inp, x)\n",
    "\n",
    "########################### Printing information ########################\n",
    "Video_Captioning_Model.summary()\n",
    "# print_model_weights(Video_Captioning_Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Video (InputLayer)           (None, 5, 10, 10, 3)      0         \n",
      "_________________________________________________________________\n",
      "Code_Seq (TimeDistributed)   (None, 5, 64)             642864    \n",
      "_________________________________________________________________\n",
      "lstm_28 (LSTM)               (None, 5, 5)              1400      \n",
      "_________________________________________________________________\n",
      "time_distributed_23 (TimeDis (None, 5, 1)              6         \n",
      "=================================================================\n",
      "Total params: 644,270\n",
      "Trainable params: 644,270\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Input_type = 'video' # 'embedding'\n",
    "\n",
    "if Input_type == 'video':\n",
    "    Inp = Input( shape=(m,n, channel), name=\"Vid_Frame\" )\n",
    "    x = Conv2D(100, (3,3), activation='relu', padding='same')(Inp)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, name='Vid_Frame_Code')(x)\n",
    "    Conv_Model = Model(Inp ,x)\n",
    "    ########################### Printing information ########################\n",
    "#     Conv_Model.summary()\n",
    "    # print_model_weights(Conv_Model)\n",
    "    \n",
    "    vid_input = Input(shape=(num_frames, m,n, channel), name=\"Video\")\n",
    "    encoded_frames = TimeDistributed(Conv_Model, name=\"Code_Seq\")(vid_input)  \n",
    "    x = LSTM(num_words, return_sequences=True)(encoded_frames)  \n",
    "    # return_sequences=False : the output will be a vector\n",
    "    x = TimeDistributed(Dense(1))(x)  \n",
    "    Video_Captioning_Model = Model(inputs=vid_input, outputs=x)\n",
    "\n",
    "\n",
    "# elif Input_type == 'embedding':\n",
    "#     Inp = Input( shape=(1,num_features),name = \"Input\" )\n",
    "#     x = LSTM(num_features, name = \"RNN_1\")(Inp) \n",
    "#     Video_Captioning_Model = Model(Inp, x)\n",
    "\n",
    "########################### Printing information ########################\n",
    "Video_Captioning_Model.summary()\n",
    "# print_model_weights(Video_Captioning_Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.10395203, -0.11779281],\n",
       "       [-0.09989359, -0.13424562],\n",
       "       [-0.07368369, -0.11910312],\n",
       "       [-0.1194683 , -0.14203915],\n",
       "       [-0.08906429, -0.06782857]], dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.rand(5,num_frames, m,n,channel)\n",
    "Video_Captioning_Model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many_to_Many (Leonard_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (5, 1, 1)\n",
      "y (5, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "## For 1 particular input\n",
    "length = 5\n",
    "# seq = np.random.rand(num_features*length)\n",
    "seq = np.array([i/float(length) for i in range(length)])\n",
    "\n",
    "n_mfcc = 1\n",
    "num_features = n_mfcc\n",
    "\n",
    "## Input \n",
    "# The input for LSTMs must be three dimensional.\n",
    "# (None=length, feature, channel)\n",
    "#    length     : duration of speech,text\n",
    "#    channel    : 1\n",
    "#    feature    : lvl of embedding\n",
    "X = seq.reshape(length, num_features, 1)\n",
    "\n",
    "## Output \n",
    "# (None=length, feature)\n",
    "#    length     : duration of speech,text\n",
    "#    channel    : 1\n",
    "#    feature    : lvl of embedding\n",
    "y = seq.reshape(length, num_features, 1)\n",
    "\n",
    "print('X', np.shape(X))\n",
    "print('y', np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X [[[ 0. ]\n",
      "  [ 0.2]\n",
      "  [ 0.4]\n",
      "  [ 0.6]\n",
      "  [ 0.8]]]\n",
      "y [[[ 0. ]\n",
      "  [ 0.2]\n",
      "  [ 0.4]\n",
      "  [ 0.6]\n",
      "  [ 0.8]]]\n"
     ]
    }
   ],
   "source": [
    "length = 5\n",
    "seq = np.array([i/float(length) for i in range(length)])\n",
    "X = seq.reshape(1, length, 1); print('X',X)\n",
    "y = seq.reshape(1, length, 1); print('y',y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', \n",
    "     recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, \n",
    "     recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, \n",
    "     recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential with TimeDistributed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare sequence\n",
    "length = 5\n",
    "seq = np.array([i/float(length) for i in range(length)])\n",
    "X = seq.reshape(1, length, 1)\n",
    "y = seq.reshape(1, length, 1)\n",
    "print('X', np.shape(X))\n",
    "print('y', np.shape(y))\n",
    "# define LSTM configuration\n",
    "n_neurons = length\n",
    "########################### Architecture ##################################\n",
    "# create LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(n_neurons, input_shape=(length, 1), return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "########################### Printing information ########################\n",
    "model.summary()\n",
    "# print_model_weights(model)\n",
    "################################# Training ##############################\n",
    "n_batch = 1\n",
    "n_epoch = 1\n",
    "# train LSTM\n",
    "model.fit(X, y, epochs=n_epoch, batch_size=n_batch, verbose=1)\n",
    "################################ Evaluation #############################\n",
    "result = model.predict(X)\n",
    "print('result\\n', result[0].T)\n",
    "print('y\\n', y[0].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
